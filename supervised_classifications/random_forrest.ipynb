{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6dd44a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (0.3.12)\n",
      "Requirement already satisfied: numpy in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: pandas in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (2.3.1)\n",
      "Requirement already satisfied: seaborn in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (3.10.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (1.7.1)\n",
      "Requirement already satisfied: packaging in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from kagglehub) (25.0)\n",
      "Requirement already satisfied: pyyaml in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from kagglehub) (2.32.4)\n",
      "Requirement already satisfied: tqdm in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from requests->kagglehub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from requests->kagglehub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/gymmnotjim/Documents/[01]_PARA/[01]_Projects/university/intro_to_ai/homework/.venv/lib/python3.13/site-packages (from requests->kagglehub) (2025.7.14)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kagglehub numpy pandas seaborn matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "530805ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTechniques for highly unbalanced\\n97%\\nDecision Tree\\n\\nraytune\\n\\nhttps://www.kaggle.com/code/albertobircoci/decisiontree-randomforest#Models\\n\\nhttps://www.kaggle.com/code/antoniosabatini/techniques-for-highly-unbalanced-data\\n\\n5 Model 97%\\nhttps://www.kaggle.com/code/mohammedezzeldean/breast-cancer-prediction-5-models-97\\n\\nhttps://www.kaggle.com/code/saeedeheydarian/recall-97-xgboost-and-randomforest\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_path = \"../Cleaned_Breast_Cancer.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# df.head()\n",
    "df.columns\n",
    "\n",
    "\"\"\"\n",
    "Techniques for highly unbalanced\n",
    "97%\n",
    "Decision Tree\n",
    "\n",
    "raytune\n",
    "\n",
    "https://www.kaggle.com/code/albertobircoci/decisiontree-randomforest#Models\n",
    "\n",
    "https://www.kaggle.com/code/antoniosabatini/techniques-for-highly-unbalanced-data\n",
    "\n",
    "5 Model 97%\n",
    "https://www.kaggle.com/code/mohammedezzeldean/breast-cancer-prediction-5-models-97\n",
    "\n",
    "https://www.kaggle.com/code/saeedeheydarian/recall-97-xgboost-and-randomforest\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aa26b750",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Survival Months\", \"Survival More Than 6 Years\"])\n",
    "y = df[\"Survival More Than 6 Years\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0108f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DecisionNode:\n",
    "  def __init__(self, impurity=None, feature_index=None, threshold=None, left=None, right=None):\n",
    "    self.left = left\n",
    "    self.right = right\n",
    "    # The largest impurity value of this node\n",
    "    self.impurity = impurity\n",
    "    # Index of the feature which make the best fit for this node.\n",
    "    self.feature_index = feature_index\n",
    "    # The threshold value for that feature to make the split.\n",
    "    self.threshold = threshold\n",
    "\n",
    "class LeafNode:\n",
    "  def __init__(self, value):\n",
    "    self.prediction_value = value\n",
    "\n",
    "class DecisionTreeClassifierFromScratch:\n",
    "  def __init__(self, min_sample_split=3, min_impurity=1e-7, max_depth=10, criterion='gini'):\n",
    "    self.root = None\n",
    "    self.min_sample_split = min_sample_split\n",
    "    self.min_impurity = min_impurity\n",
    "    self.max_depth = max_depth\n",
    "    self.impurity_function = self._claculate_information_gain\n",
    "    if criterion == 'entropy':\n",
    "      self.criterion = self._entropy\n",
    "      self.criterion_name = criterion\n",
    "    else:\n",
    "      self.criterion = self._gini_index\n",
    "      self.criterion_name = 'gini'\n",
    "\n",
    "  def _gini_index(self, y):\n",
    "    gini = 1\n",
    "    unique_value = np.unique(y)\n",
    "    for val in unique_value:\n",
    "      # probability of that class.\n",
    "      p = np.sum(y == val) / len(y)\n",
    "      gini += -np.square(p)\n",
    "    return gini\n",
    "\n",
    "  def _entropy(self, y):\n",
    "    entropy = 0\n",
    "    unique_value = np.unique(y)\n",
    "    for val in unique_value:\n",
    "      # probability of that class.\n",
    "      p = np.sum(y == val) / len(y)\n",
    "      entropy += -p * np.log2(p)\n",
    "    return entropy\n",
    "\n",
    "  def _claculate_information_gain(self, y, y1, y2):\n",
    "    # :param y: target value.\n",
    "    # :param y1: target value for dataset in the true split/right branch.\n",
    "    # :param y2: target value for dataset in the false split/left branch.\n",
    "\n",
    "    # propobility of true values.\n",
    "    p = len(y1) / len(y)\n",
    "    info_gain = self.criterion(y) - p * self.criterion(y1) - (1 - p) * self.criterion(y2)\n",
    "    return info_gain\n",
    "\n",
    "  def _leaf_value_calculation(self, y):\n",
    "    most_frequent_label = None\n",
    "    max_count = 0\n",
    "    unique_labels = np.unique(y)\n",
    "    # iterate over all the unique values and find their frequentcy count.\n",
    "    for label in unique_labels:\n",
    "      count = len( y[y == label])\n",
    "      if count > max_count:\n",
    "        most_frequent_label = label\n",
    "        max_count = count\n",
    "    return most_frequent_label\n",
    "\n",
    "  def _partition_dataset(self, Xy, feature_index, threshold):\n",
    "    col = Xy[:, feature_index]\n",
    "    X_1 = Xy[col >= threshold]\n",
    "    X_2 = Xy[col < threshold]\n",
    "\n",
    "    return X_1, X_2\n",
    "\n",
    "  def _find_best_split(self, Xy):\n",
    "    best_question = tuple()\n",
    "    best_datasplit = {}\n",
    "    largest_impurity = 0\n",
    "    n_features = (Xy.shape[1] - 1)\n",
    "    # iterate over all the features.\n",
    "    for feature_index in range(n_features):\n",
    "      # find the unique values in that feature.\n",
    "      unique_value = set(s for s in Xy[:,feature_index])\n",
    "      # iterate over all the unique values to find the impurity.\n",
    "      for threshold in unique_value:\n",
    "        # split the dataset based on the feature value.\n",
    "        true_xy, false_xy = self._partition_dataset(Xy, feature_index, threshold)\n",
    "\n",
    "        # skip the node which has any on type 0. because this means it is already pure.\n",
    "        if len(true_xy) > 0 and len(false_xy) > 0:\n",
    "          # find the y values.\n",
    "          y = Xy[:, -1]\n",
    "          true_y = true_xy[:, -1]\n",
    "          false_y = false_xy[:, -1]\n",
    "          # calculate the impurity function.\n",
    "          impurity = self.impurity_function(y, true_y, false_y)\n",
    "\n",
    "          # if the calculated impurity is larger than save this value for comaparition (highest gain).\n",
    "          if impurity > largest_impurity:\n",
    "            largest_impurity = impurity\n",
    "            best_question = (feature_index, threshold)\n",
    "            best_datasplit = {\n",
    "              \"leftX\": true_xy[:, :n_features],   # X of left subtree\n",
    "              \"lefty\": true_xy[:, n_features:],   # y of left subtree\n",
    "              \"rightX\": false_xy[:, :n_features],  # X of right subtree\n",
    "              \"righty\": false_xy[:, n_features:]   # y of right subtree\n",
    "            }\n",
    "\n",
    "    return largest_impurity, best_question, best_datasplit\n",
    "\n",
    "  def _build_tree(self, X, y, current_depth=0):\n",
    "    n_samples , n_features = X.shape\n",
    "    # Add y as last column of X\n",
    "    Xy = np.column_stack((X, y))\n",
    "    # find the Information gain on each feature each values and return the question which splits the data very well\n",
    "    if (n_samples >= self.min_sample_split) and (current_depth < self.max_depth):\n",
    "      # find the best split/ which question split the data well.\n",
    "      impurity, question, best_datasplit = self._find_best_split(Xy)\n",
    "      if impurity > self.min_impurity:\n",
    "        # Build subtrees for the right and left branch.\n",
    "        true_branch = self._build_tree(best_datasplit[\"leftX\"], best_datasplit[\"lefty\"], current_depth + 1)\n",
    "        false_branch = self._build_tree(best_datasplit[\"rightX\"], best_datasplit[\"righty\"], current_depth + 1)\n",
    "        return DecisionNode(impurity=impurity, feature_index=question[0], threshold=question[1],\n",
    "                            left=true_branch, right=false_branch)\n",
    "\n",
    "    leaf_value = self._leaf_value_calculation(y)\n",
    "    return LeafNode(value=leaf_value)\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    self.root = self._build_tree(X, y, current_depth=0)\n",
    "\n",
    "  def predict_sample(self, x, tree=None):\n",
    "    if isinstance(tree , LeafNode):\n",
    "      return tree.prediction_value\n",
    "\n",
    "    if tree is None:\n",
    "      tree = self.root\n",
    "    feature_value = x[tree.feature_index]\n",
    "    branch = tree.right\n",
    "\n",
    "    if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "      if feature_value >= tree.threshold:\n",
    "        branch = tree.left\n",
    "    elif feature_value == tree.threshold:\n",
    "      branch = tree.left\n",
    "\n",
    "    return self.predict_sample(x, branch)\n",
    "\n",
    "  def predict(self, test_X):\n",
    "    x = np.array(test_X)\n",
    "    y_pred = [self.predict_sample(sample) for sample in x]\n",
    "    y_pred = np.array(y_pred)\n",
    "    return y_pred\n",
    "\n",
    "  def draw_tree(self):\n",
    "    self._draw_tree(self.root)\n",
    "\n",
    "  def _draw_tree(self, tree = None, indentation = \" \", depth=0):\n",
    "    if isinstance(tree , LeafNode):\n",
    "      print(indentation,\"The predicted value -->\", tree.prediction_value)\n",
    "      return\n",
    "    else:\n",
    "      print(indentation,f\"({depth}) Is {tree.feature_index}>={tree.threshold}?\"\n",
    "            f\": {self.criterion_name}:{tree.impurity:.2f}\")\n",
    "      if tree.left is not None:\n",
    "          print (indentation + '----- True branch :)')\n",
    "          self._draw_tree(tree.left, indentation + \"  \", depth+1)\n",
    "      if tree.right is not None:\n",
    "          print (indentation + '----- False branch :)')\n",
    "          self._draw_tree(tree.right, indentation + \"  \", depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0e17b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifierFromScratch:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_feature=None,\n",
    "        n_trees=100,\n",
    "        min_sample_split=2,\n",
    "        min_impurity=1e-7,\n",
    "        max_depth=10,\n",
    "        criterion=\"gini\",\n",
    "    ):\n",
    "        # Initialize the trees.\n",
    "        self.trees = []\n",
    "        for _ in range(n_trees):\n",
    "            self.trees.append(\n",
    "                DecisionTreeClassifierFromScratch(\n",
    "                    min_sample_split=min_sample_split,\n",
    "                    min_impurity=min_impurity,\n",
    "                    max_depth=max_depth,\n",
    "                    criterion=criterion,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.tree_feature_indexes = []\n",
    "        # Number of trees/estimetors.\n",
    "        self.n_estimators = n_trees\n",
    "        # How many features can be used for a tree from the whole features.\n",
    "        self.max_features = max_feature\n",
    "        # Aggication function to find the prediction.\n",
    "        self.prediction_aggrigation_calculation = self._maximum_vote_calculation\n",
    "\n",
    "    def _maximum_vote_calculation(self, y_preds):\n",
    "        # Find which prediction class has higest frequency in all tree prediction for each sample.\n",
    "        # create a empty array to store the prediction.\n",
    "        y_pred = np.empty((y_preds.shape[0], 1))\n",
    "        # iterate over all the data samples.\n",
    "        for i, sample_predictions in enumerate(y_preds):\n",
    "            y_pred[i] = np.bincount(sample_predictions.astype(\"int\")).argmax()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def _make_random_subset(self, X, y, n_subsets, replacement=True):\n",
    "        # Create a random subset of dataset with/without replacement.\n",
    "        subset = []\n",
    "        # use 100% of data when replacement is true , use 50% otherwise.\n",
    "        sample_size = X.shape[0] if replacement else (X.shape[0] // 2)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = np.column_stack((X, y))\n",
    "        np.random.shuffle(Xy)\n",
    "        # Select randome subset of data with replacement.\n",
    "        for i in range(n_subsets):\n",
    "            index = np.random.choice(\n",
    "                range(sample_size),\n",
    "                size=np.shape(range(sample_size)),\n",
    "                replace=replacement,\n",
    "            )\n",
    "            X = Xy[index][:, :-1]\n",
    "            y = Xy[index][:, -1]\n",
    "            subset.append({\"X\": X, \"y\": y})\n",
    "        return subset\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # if the max_features is not given then select it as square root of no on feature availabe.\n",
    "        n_features = X.shape[1]\n",
    "        if self.max_features == None:\n",
    "            self.max_features = int(round(np.sqrt(n_features)))\n",
    "\n",
    "        # Split the dataset into number of subsets equal to n_estimators.\n",
    "        subsets = self._make_random_subset(X, y, self.n_estimators)\n",
    "\n",
    "        for i, subset in enumerate(subsets):\n",
    "            X_subset, y_subset = subset[\"X\"], subset[\"y\"]\n",
    "            # select a random sucset of features for each tree. This is called feature bagging.\n",
    "            idx = np.random.choice(\n",
    "                range(n_features), size=self.max_features, replace=True\n",
    "            )\n",
    "            # track this for prediction.\n",
    "            self.tree_feature_indexes.append(idx)\n",
    "            # Get the X with the selected features only.\n",
    "            X_subset = X_subset[:, idx]\n",
    "\n",
    "            # change the y_subet to i dimentional array.\n",
    "            y_subset = np.expand_dims(y_subset, axis=1)\n",
    "            # build the model with selected features and selected random subset from dataset.\n",
    "            self.trees[i].fit(X_subset, y_subset)\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        y_preds = np.empty((test_X.shape[0], self.n_estimators))\n",
    "        # find the prediction from each tree for each samples\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            features_index = self.tree_feature_indexes[i]\n",
    "\n",
    "            # test_X = (slice(None, None, None), array([8, 6, 0]))\n",
    "\n",
    "            X_selected_features = test_X[:, features_index]\n",
    "            if isinstance(tree, DecisionTreeClassifierFromScratch):\n",
    "                y_preds[:, i] = tree.predict(X_selected_features).reshape((-1,))\n",
    "            else:\n",
    "                y_preds[:, i] = tree.predict(X_selected_features)\n",
    "        # find the aggregated output.\n",
    "        y_pred = self.prediction_aggrigation_calculation(y_preds)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1fd48a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5969184890656064"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestClassifierFromScratch(n_trees=10, criterion=\"entropy\")\n",
    "rf.fit(X, y)\n",
    "sc_rf_pred = rf.predict(X.to_numpy())\n",
    "\n",
    "accuracy_score(y, sc_rf_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Intro-to-ai",
   "language": "python",
   "name": "myproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
